---
layout: ../layouts/Projects.astro
projects:
  Code:
    - name: GUIDE
      link: https://github.com/XThomasBU/GUIDE
      desc: Code for "What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization"
      tech:
        - i-logos:python
      repo: XThomasBU/GUIDE
    - name: revelio
      link: https://github.com/revelio-diffusion/revelio
      desc: Code for "Revelio, Interpreting and Leveraging Visual Semantic Information in Diffusion Models"
      tech:
        - i-logos:python
      repo: revelio-diffusion/revelio
    - name: scope-diffusers
      link: https://github.com/Ketansuhaas/scope-diffusers
      desc: Code for "Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models"
      tech:
        - i-logos:python
      repo: Ketansuhaas/scope-diffusers
    - name: edubotics-core
      link: https://github.com/edubotics-ai/edubotics-core
      desc: edubotics-core is an open-source Python library that allows developers to build LLM-based chatbots efficiently. It provides a comprehensive set of core modules for vector storage, retrieval, processing, with more to come.
      tech:
        - i-logos:python
      repo: edubotics-ai/edubotics-core
    - name: sonar
      link: https://github.com/aidecentralized/sonar
      desc: SONAR - Self-Organizing Network of Aggregated Representations
      tech:
        - i-logos:python
      repo: aidecentralized/sonar
    - name: AdaClust_DomainBed
      link: https://github.com/xavierohan/AdaClust_DomainBed
      desc: Code for AdaClust, Adaptive Methods for Aggregated Domain Generalization
      tech:
        - i-logos:python
      repo: xavierohan/AdaClust_DomainBed
    - name: diversity_vs_recognizability
      link: https://github.com/serre-lab/diversity_vs_recognizability
      desc: Code for Diversity vs. Recognizability, Human-like generalization in one-shot generative models
      tech:
        - i-logos:python
      repo: serre-lab/diversity_vs_recognizability
---

### Generative Action Tell-Tales: Assessing human motion in synthesized videos

<div class="webpage-embed-container">
  <div class="webpage-embed-header">
    <span class="webpage-embed-title">Generative Action Tell-Tales: Assessing human motion in synthesized videos</span>
    <a 
      href="https://xthomasbu.github.io/video-gen-evals/" 
      target="_blank" 
      rel="noopener noreferrer"
      class="webpage-embed-link"
      title="Open in new tab"
    >
      <span class="webpage-embed-icon">↗</span>
    </a>
  </div>
  <div class="webpage-embed-wrapper">
    <iframe
      src="https://xthomasbu.github.io/video-gen-evals/"
      class="webpage-embed-iframe"
      title="Generative Action Tell-Tales: Assessing human motion in synthesized videos"
      loading="lazy"
      allow="fullscreen"
      sandbox="allow-same-origin allow-scripts allow-popups allow-forms"
    ></iframe>
  </div>
</div>

### Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs

<div class="webpage-embed-container">
  <div class="webpage-embed-header">
    <span class="webpage-embed-title">Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs</span>
    <a 
      href="https://cskyl.github.io/MMA-Bench/" 
      target="_blank" 
      rel="noopener noreferrer"
      class="webpage-embed-link"
      title="Open in new tab"
    >
      <span class="webpage-embed-icon">↗</span>
    </a>
  </div>
  <div class="webpage-embed-wrapper">
    <iframe
      src="https://cskyl.github.io/MMA-Bench/"
      class="webpage-embed-iframe"
      title="Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs"
      loading="lazy"
      allow="fullscreen"
      sandbox="allow-same-origin allow-scripts allow-popups allow-forms"
    ></iframe>
  </div>
</div>

### What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization

<div class="webpage-embed-container">
  <div class="webpage-embed-header">
    <span class="webpage-embed-title">What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization</span>
    <a 
      href="https://xthomasbu.github.io/GUIDE/" 
      target="_blank" 
      rel="noopener noreferrer"
      class="webpage-embed-link"
      title="Open in new tab"
    >
      <span class="webpage-embed-icon">↗</span>
    </a>
  </div>
  <div class="webpage-embed-wrapper">
    <iframe
      src="https://xthomasbu.github.io/GUIDE/"
      class="webpage-embed-iframe"
      title="What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization"
      loading="lazy"
      allow="fullscreen"
      sandbox="allow-same-origin allow-scripts allow-popups allow-forms"
    ></iframe>
  </div>
</div>

### Revelio: Interpreting and leveraging semantic information in diffusion models

<div class="webpage-embed-container">
  <div class="webpage-embed-header">
    <span class="webpage-embed-title">Revelio: Interpreting and leveraging semantic information in diffusion models</span>
    <a 
      href="https://revelio-diffusion.github.io/revelio/" 
      target="_blank" 
      rel="noopener noreferrer"
      class="webpage-embed-link"
      title="Open in new tab"
    >
      <span class="webpage-embed-icon">↗</span>
    </a>
  </div>
  <div class="webpage-embed-wrapper">
    <iframe
      src="https://revelio-diffusion.github.io/revelio/"
      class="webpage-embed-iframe"
      title="Revelio: Interpreting and leveraging semantic information in diffusion models"
      loading="lazy"
      allow="fullscreen"
      sandbox="allow-same-origin allow-scripts allow-popups allow-forms"
    ></iframe>
  </div>
</div>

### Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models

<div class="webpage-embed-container">
  <div class="webpage-embed-header">
    <span class="webpage-embed-title">Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models</span>
    <a 
      href="https://ketansuhaas.github.io/scope-diffusers" 
      target="_blank" 
      rel="noopener noreferrer"
      class="webpage-embed-link"
      title="Open in new tab"
    >
      <span class="webpage-embed-icon">↗</span>
    </a>
  </div>
  <div class="webpage-embed-wrapper">
    <iframe
      src="https://ketansuhaas.github.io/scope-diffusers"
      class="webpage-embed-iframe"
      title="Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models"
      loading="lazy"
      allow="fullscreen"
      sandbox="allow-same-origin allow-scripts allow-popups allow-forms"
    ></iframe>
  </div>
</div>

## edubotics-core: An Open-Source Python Library for Building LLM-based Applications

edubotics-core is an open-source Python library that allows developers to build LLM-based chatbots efficiently. It provides a comprehensive set of core modules for vector storage, retrieval, processing, with more to come.

<img src="https://github.com/edubotics-ai/.github/blob/main/assets/images/edubot-mascot.png?raw=true" class="paper-images"  width="30%" height="30%"/>

**Installation:**

```bash
pip install edubotics-core
```

Code: https://github.com/edubotics-ai/edubotics-core

Full Documentation: http://docs.edubotics.ai

Applications built with edubotics-core: https://github.com/edubotics-ai/edubotics-app

<img src="/img/my_imgs/edubotics_1.png" class="paper-images"  width="100%" height="100%"/>
<img src="/img/my_imgs/edubotics_2.png" class="paper-images"  width="100%" height="100%"/>
&nbsp;

## MAViC: Multimodal Active Learning for Video Captioning

A large number of annotated video-caption pairs are required for training video captioning models, resulting in high annotation costs. Active learning can be instrumental in reducing these annotation requirements. However, active learning for video captioning is challenging because multiple semantically similar captions are valid for a video, resulting in high entropy outputs even for less-informative samples. Moreover, video captioning algorithms are multimodal in nature with a visual encoder and language decoder. Further, the sequential and combinatorial nature of the output makes the problem even more challenging. In this work, we introduce MAViC which leverages our proposed Multimodal Semantics Aware Sequential Entropy (M-SASE) based acquisition function to address the challenges of active learning approaches for video captioning. Our approach integrates semantic similarity and uncertainty of both visual and language dimensions in the acquisition function. Our detailed experiments empirically demonstrate the efficacy of M-SASE for active learning for video captioning and improve on the baselines by a large margin.

<img src="/img/my_imgs/MAVIC.png" class="paper-images"  width="100%" height="100%"/>

Full Paper: [https://arxiv.org/abs/2212.11109](https://arxiv.org/abs/2212.11109)

&nbsp;

## AdaClust: Adaptive Methods for Aggregated Domain Generalization

Domain generalization involves learning a classifier from a heterogeneous collection of training sources such that it generalizes to data drawn from similar unknown target domains, with applications in large-scale learning and personalized inference. In many settings, privacy concerns prohibit obtaining domain labels for the training data samples, and instead only have an aggregated collection of training points. Existing approaches that utilize domain labels to create domain-invariant feature representations are inapplicable in this setting, requiring alternative approaches to learn generalizable classifiers. In this work, we propose a domain-adaptive approach to this problem, which operates in two steps: (a) we cluster training data within a carefully chosen feature space to create pseudo-domains, and (b) using these pseudo-domains we learn a domain-adaptive classifier that makes predictions using information about both the input and the pseudo-domain it belongs to. Our approach achieves state-of-the-art performance on a variety of domain generalization benchmarks without using domain labels whatsoever. Furthermore, we provide novel theoretical guarantees on domain generalization  in using cluster information. Our approach is amenable to ensemble-based methods and provides substantial gains even on large-scale benchmark datasets.

<img src="/img/my_imgs/main_cvpr22_new.jpg" class="paper-images"/>

Full Paper: [https://arxiv.org/abs/2112.04766](https://arxiv.org/abs/2112.04766) 

Code: [https://github.com/xavierohan/AdaClust_DomainBed](https://github.com/xavierohan/AdaClust_DomainBed)

&nbsp;

## Diversity vs. Recognizability: Human-like generalization in one-shot generative models

Robust generalization to new concepts has long remained a distinctive feature of human intelligence. However, recent progress in deep generative models has now led to neural architectures capable of synthesizing novel instances of unknown visual concepts from a single training example. Yet, a more precise comparison between these models and humans is not possible because existing performance metrics for generative models (i.e., FID, IS, likelihood) are not appropriate for the one-shot generation scenario. Here, we propose a new framework to evaluate one-shot generative models along two axes: sample recognizability vs. diversity  (i.e., intra-class variability). Using this framework, we perform a systematic evaluation of representative one-shot generative models on the Omniglot handwritten dataset. We first show that GAN-like and VAE-like models fall on opposite ends of the diversity-recognizability space. Extensive analyses of the effect of key model parameters further revealed that spatial attention and context integration have a linear contribution to the diversity-recognizability trade-off. In contrast, disentanglement transports the model along a parabolic curve that could be used to maximize recognizability. Using the diversity-recognizability framework, we were able to identify models and parameters that closely approximate human data.

<div style="display: flex; justify-content: space-between; align-items: center;">
  <img src="/img/my_imgs/Fig_serrelab_resized.png" class="paper-images" width="50%" height="50%"/>
  <img src="/img/my_imgs/div_vs_rec2.png" class="paper-images" width="50%" height="50%"/>
</div>


Full Paper: [https://arxiv.org/abs/2205.10370](https://arxiv.org/abs/2205.10370) 

Code: [https://github.com/serre-lab/diversity_vs_recognizability](https://github.com/serre-lab/diversity_vs_recognizability)
